{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d01217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/MyPythonEnvNew/lib/python3.10/site-packages/pyproj/network.py:59: UserWarning: pyproj unable to set PROJ database path.\n",
      "  _set_context_ca_bundle_path(ca_bundle_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "os.environ[\"BASE_DIR\"] = \"/work/FAC/FGSE/IDYST/tbeucler/downscaling\"\n",
    "BASE_DIR = os.environ[\"BASE_DIR\"]\n",
    "import json\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7435896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../../Scripts/Functions/Climate_Indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de479224",
   "metadata": {},
   "source": [
    "Loading the saved quick check trained model from Downscaling_Models/UNet_Deterministic_training_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39c5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(\"full_best_model_huber_pretraining_FULL_RLOP.pth\")\n",
    "training_checkpoint =torch.load(model_path,map_location=torch.device('cpu')) #Moving model to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c587ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Checking the parameters and keys\n",
    "print(type(training_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15f4478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "model_state_dict\n",
      "optimizer_state_dict\n",
      "loss\n"
     ]
    }
   ],
   "source": [
    "#Checking all model parameters \n",
    "for key in training_checkpoint.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing UNet class from Unet.py\n",
    "sys.path.append(os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/models_UNet/UNet_Deterministic_Pretraining_Dataset\"))\n",
    "from UNet import UNet #Importing Unet class\n",
    "from Downscaling_Dataset_Prep import DownscalingDataset #for creating paired frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8386844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06b1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (Encoder1): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder2): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder3): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder4): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Decoder1): Decoder_Block(\n",
       "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder2): Decoder_Block(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder3): Decoder_Block(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder4): Decoder_Block(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outputs): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance= UNet(in_channels=5, out_channels=4)\n",
    "model_instance.load_state_dict(training_checkpoint[\"model_state_dict\"])\n",
    "model_instance.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7315a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled datasets for test set (2011-2020) :loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12a3b452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling params loading from the .json files\n",
    "scaling_dir = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/Pretraining_Chronological_Dataset\")\n",
    "rhiresd_params = json.load(open(os.path.join(scaling_dir, \"precip_scaling_params_chronological.json\")))\n",
    "tabsd_params   = json.load(open(os.path.join(scaling_dir, \"temp_scaling_params_chronological.json\")))\n",
    "tmind_params   = json.load(open(os.path.join(scaling_dir, \"tmin_scaling_params_chronological.json\")))\n",
    "tmaxd_params   = json.load(open(os.path.join(scaling_dir, \"tmax_scaling_params_chronological.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f840c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization functions : now using parameters from the loaded JSON files\n",
    "\n",
    "def norm_precip(x, params):\n",
    "    return (x - params[\"min\"]) / (params[\"max\"] - params[\"min\"])\n",
    "\n",
    "def norm_temp(x, params):\n",
    "    return (x - params[\"mean\"]) / params[\"std\"]\n",
    "\n",
    "def norm_tmin(x, params):\n",
    "    return (x - params[\"mean\"]) / params[\"std\"]\n",
    "\n",
    "def norm_tmax(x, params):\n",
    "    return (x - params[\"mean\"]) / params[\"std\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c6ca0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip=xr.open_dataset(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Processing_and_Analysis_Scripts/data_1971_2023/HR_files_full/RhiresD_1971_2023.nc\")\n",
    "temp=xr.open_dataset(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Processing_and_Analysis_Scripts/data_1971_2023/HR_files_full/TabsD_1971_2023.nc\")\n",
    "tmin=xr.open_dataset(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Processing_and_Analysis_Scripts/data_1971_2023/HR_files_full/TminD_1971_2023.nc\")\n",
    "tmax=xr.open_dataset(\"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Processing_and_Analysis_Scripts/data_1971_2023/HR_files_full/TmaxD_1971_2023.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b842a6e1",
   "metadata": {},
   "source": [
    "Renaming has to be done because the model was trained on the long time series and is being twstd on the observations of the shorter time series "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14c1b1a",
   "metadata": {},
   "source": [
    "Inputs Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b6da0229",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_input = xr.open_dataset(\n",
    "    \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/Training_Chronological_Dataset/RhiresD_step3_interp.nc\"\n",
    ").sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"RhiresD\"].chunk({\"time\": 100}).rename(\"precip\")\n",
    "temp_input = xr.open_dataset(\n",
    "    \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/Training_Chronological_Dataset/TabsD_step3_interp.nc\"\n",
    ").sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TabsD\"].chunk({\"time\": 100}).rename(\"temp\")\n",
    "tmin_input = xr.open_dataset(\n",
    "    \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/Training_Chronological_Dataset/TminD_step3_interp.nc\"\n",
    ").sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TminD\"].chunk({\"time\": 100}).rename(\"tmin\")\n",
    "tmax_input = xr.open_dataset(\n",
    "    \"/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/Training_Chronological_Dataset/TmaxD_step3_interp.nc\"\n",
    ").sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TmaxD\"].chunk({\"time\": 100}).rename(\"tmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9727a43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize using loaded JSON parameters\n",
    "precip_input = norm_precip(precip_input, rhiresd_params).rename(\"precip\")\n",
    "temp_input   = norm_temp(temp_input, tabsd_params).rename(\"temp\")\n",
    "tmin_input   = norm_tmin(tmin_input, tmind_params).rename(\"tmin\")\n",
    "tmax_input   = norm_tmax(tmax_input, tmaxd_params).rename(\"tmax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7249659f",
   "metadata": {},
   "source": [
    "Targets prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84656337",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_target = precip_target = norm_precip(\n",
    "    precip.sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"RhiresD\"].chunk({\"time\": 100}),\n",
    "    rhiresd_params).rename(\"precip\")\n",
    "temp_target = norm_temp(\n",
    "    temp.sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TabsD\"].chunk({\"time\": 100}),\n",
    "    tabsd_params).rename(\"temp\")\n",
    "tmin_target = norm_tmin(\n",
    "    tmin.sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TminD\"].chunk({\"time\": 100}),\n",
    "    tmind_params).rename(\"tmin\")\n",
    "tmax_target = norm_tmax(\n",
    "    tmax.sel(time=slice(\"2011-01-01\", \"2020-12-31\"))[\"TmaxD\"].chunk({\"time\": 100}),\n",
    "    tmaxd_params).rename(\"tmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f45e4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the paired feature-target dataset; first loading individual and coverting them into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3dbd20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config used for training \n",
    "config_path = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/models_UNet/UNet_Deterministic_Pretraining_Dataset/config.yaml\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "871ecfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation\n",
    "elevation_path = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/elevation.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294e91",
   "metadata": {},
   "source": [
    "merging DS before creating pairs,,,,wont work for individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e6847b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded elevation from /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/elevation.tif, shape: (255, 387)\n"
     ]
    }
   ],
   "source": [
    "inputs_merged = xr.merge([precip_input, temp_input, tmin_input, tmax_input])\n",
    "targets_merged = xr.merge([precip_target, temp_target, tmin_target, tmax_target])\n",
    "\n",
    "ds = DownscalingDataset(inputs_merged, targets_merged, config, elevation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "718f9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 3653 samples\n"
     ]
    }
   ],
   "source": [
    "#Checking shape of the ds instance \n",
    "print(f\"Dataset shape: {ds.__len__()} samples\") #Number of samples in the test set #Test set across configs remain the same "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fdc42b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([5, 240, 370])\n",
      "Target image shape: torch.Size([4, 240, 370])\n"
     ]
    }
   ],
   "source": [
    "#Checking shape of a random sample\n",
    "input_img,target_img= ds[15] #14 th sample\n",
    "print(f\"Input image shape: {input_img.shape}\")\n",
    "print(f\"Target image shape: {target_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "504d7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "paired_ds = DataLoader(ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "30d001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= 2008\n",
    "input_img, target_img =ds[idx]\n",
    "input_img= input_img.unsqueeze(0) # Adding batch dimension\n",
    "date=str(inputs_merged.time.values[idx]) #What date?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eabc5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.eval()\n",
    "with torch.no_grad():\n",
    "    pred_img = model_instance(input_img).squeeze(0).cpu().numpy()  \n",
    "target_img = target_img.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8be976b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale_precip(x, min_val, max_val):\n",
    "    return x * (max_val - min_val) + min_val\n",
    "\n",
    "def descale_temp(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a5f9bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img_denorm = np.empty_like(pred_img)\n",
    "target_img_denorm = np.empty_like(target_img)\n",
    "\n",
    "pred_img_denorm[0] = descale_precip(pred_img[0], rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "pred_img_denorm[1] = descale_temp(pred_img[1], tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "pred_img_denorm[2] = descale_temp(pred_img[2], tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "pred_img_denorm[3] = descale_temp(pred_img[3], tmaxd_params[\"mean\"], tmaxd_params[\"std\"])\n",
    "\n",
    "target_img_denorm[0] = descale_precip(target_img[0], rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "target_img_denorm[1] = descale_temp(target_img[1], tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "target_img_denorm[2] = descale_temp(target_img[2], tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "target_img_denorm[3] = descale_temp(target_img[3], tmaxd_params[\"mean\"], tmaxd_params[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68805c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = inputs_merged.lat.values\n",
    "lon = inputs_merged.lon.values\n",
    "var_names = [\"precip\", \"temp\", \"tmin\", \"tmax\"]\n",
    "\n",
    "# Define fixed colorbar limits for each variable\n",
    "vmin_dict = {\"precip\": 0, \"temp\": -5, \"tmin\": -10, \"tmax\": -10}\n",
    "vmax_dict = {\"precip\": 50, \"temp\": 25, \"tmin\": 20, \"tmax\": 30}\n",
    "\n",
    "swiss_extent = [5.9, 10.5, 45.7, 47.9]  \n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "input_img = input_img.squeeze(0)  \n",
    "for i, var in enumerate(var_names):\n",
    "    # Get input image for this variable (input_img shape: (5, H, W))\n",
    "    input_img_denorm = None\n",
    "    if var == \"precip\":\n",
    "        input_img_denorm = descale_precip(input_img[0].cpu().numpy(), rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "    elif var == \"temp\":\n",
    "        input_img_denorm = descale_temp(input_img[1].cpu().numpy(), tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "    elif var == \"tmin\":\n",
    "        input_img_denorm = descale_temp(input_img[2].cpu().numpy(), tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "    elif var == \"tmax\":\n",
    "        input_img_denorm = descale_temp(input_img[3].cpu().numpy(), tmaxd_params[\"mean\"], tmaxd_params[\"std\"])\n",
    "\n",
    "    # Use fixed vmin/vmax for each variable\n",
    "    vmin = vmin_dict[var]\n",
    "    vmax = vmax_dict[var]\n",
    "\n",
    "    # Input\n",
    "    ax = axes[i, 0]\n",
    "    im = ax.pcolormesh(lon, lat, input_img_denorm, cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Input {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Target\n",
    "    ax = axes[i, 1]\n",
    "    im = ax.pcolormesh(lon, lat, target_img_denorm[i], cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Target {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Predicted\n",
    "    ax = axes[i, 2]\n",
    "    im = ax.pcolormesh(lon, lat, pred_img_denorm[i], cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Predicted {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "plt.suptitle(f\"Inputs(bicubic) , Target(HR) and Predictions from the 1770s time series : {date}\", fontsize=14, y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR+\"/sasthana/Downscaling/Processing_and_Analysis_Scripts/Outputs/1_July_2016_unet_1771_2020.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b3c74a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
