{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60d01217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/MyPythonEnvNew/lib/python3.10/site-packages/pyproj/network.py:59: UserWarning: pyproj unable to set PROJ database path.\n",
      "  _set_context_ca_bundle_path(ca_bundle_path)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "os.environ[\"BASE_DIR\"] = \"/work/FAC/FGSE/IDYST/tbeucler/downscaling\"\n",
    "BASE_DIR = os.environ[\"BASE_DIR\"]\n",
    "import json\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7435896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"../../Scripts/Functions/Climate_Indices\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de479224",
   "metadata": {},
   "source": [
    "Loading the saved quick check trained model from Downscaling_Models/UNet_Deterministic_training_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b39c5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = os.path.join(BASE_DIR,\"sasthana/Downscaling/Downscaling_Models/models_UNet/UNet_Deterministic_Training_Dataset/full_best_model_huber_FULL_RLOP.pth\")\n",
    "training_checkpoint =torch.load(model_path,map_location=torch.device('cpu')) #Moving model to CPU "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c587ea78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "#Checking the parameters and keys\n",
    "print(type(training_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15f4478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch\n",
      "model_state_dict\n",
      "optimizer_state_dict\n",
      "loss\n"
     ]
    }
   ],
   "source": [
    "#Checking all model parameters \n",
    "for key in training_checkpoint.keys():\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53c8e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing UNet class from Unet.py\n",
    "sys.path.append(os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/models_UNet/UNet_Deterministic_Training_Dataset\"))\n",
    "from UNet import UNet #Importing Unet class\n",
    "from Downscaling_Dataset_Prep import DownscalingDataset #for creating paired frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8386844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b06b1c29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNet(\n",
       "  (Encoder1): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder2): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder3): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (Encoder4): Encoder_Block(\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (bottleneck): DoubleConv(\n",
       "    (conv): Sequential(\n",
       "      (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (Decoder1): Decoder_Block(\n",
       "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder2): Decoder_Block(\n",
       "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder3): Decoder_Block(\n",
       "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (Decoder4): Decoder_Block(\n",
       "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "    (conv): DoubleConv(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU(inplace=True)\n",
       "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (5): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (outputs): Conv2d(64, 4, kernel_size=(1, 1), stride=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_instance= UNet(in_channels=5, out_channels=4)\n",
    "model_instance.load_state_dict(training_checkpoint[\"model_state_dict\"])\n",
    "model_instance.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b7315a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scaled datasets for test set (2011-2020) :loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ed96f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_input = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"RhiresD_input_test_chronological_scaled.nc\"))\n",
    "temp_input = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TabsD_input_test_chronological_scaled.nc\"))\n",
    "tmin_input= xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TminD_input_test_chronological_scaled.nc\"))\n",
    "tmax_input= xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TmaxD_input_test_chronological_scaled.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ee32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_target = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"RhiresD_target_test_chronological_scaled.nc\"))\n",
    "temp_target = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TabsD_target_test_chronological_scaled.nc\"))\n",
    "tmin_target = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TminD_target_test_chronological_scaled.nc\"))\n",
    "tmax_target = xr.open_dataset(os.path.join(BASE_DIR, \"sasthana\", \"Downscaling\", \"Downscaling_Models\", \"Training_Chronological_Dataset\", \"TmaxD_target_test_chronological_scaled.nc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45e4557",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the paired feature-target dataset; first loading individual and coverting them into images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3dbd20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config used for training \n",
    "config_path = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/models_UNet/UNet_Deterministic_Training_Dataset/config.yaml\")\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "871ecfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# elevation\n",
    "elevation_path = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/elevation.tif\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5294e91",
   "metadata": {},
   "source": [
    "merging DS before creating pairs,,,,wont work for individual files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6847b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded elevation from /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/elevation.tif, shape: (255, 387)\n"
     ]
    }
   ],
   "source": [
    "inputs_merged = xr.merge([precip_input, temp_input, tmin_input, tmax_input])\n",
    "targets_merged = xr.merge([precip_target, temp_target, tmin_target, tmax_target])\n",
    "\n",
    "ds = DownscalingDataset(inputs_merged, targets_merged, config, elevation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "718f9d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: 3653 samples\n"
     ]
    }
   ],
   "source": [
    "#Checking shape of the ds instance \n",
    "print(f\"Dataset shape: {ds.__len__()} samples\") #Number of samples in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fdc42b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input image shape: torch.Size([5, 240, 370])\n",
      "Target image shape: torch.Size([4, 240, 370])\n"
     ]
    }
   ],
   "source": [
    "#Checking shape of a random sample\n",
    "input_img,target_img= ds[15] #14 th sample\n",
    "print(f\"Input image shape: {input_img.shape}\")\n",
    "print(f\"Target image shape: {target_img.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504d7b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "paired_ds = DataLoader(ds, batch_size=1, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3336da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 240, 370])\n",
      "torch.Size([1, 4, 240, 370])\n"
     ]
    }
   ],
   "source": [
    "for input_batch, target_batch in paired_ds:\n",
    "    print(input_batch.shape)  # (1, 5, H, W)\n",
    "    print(target_batch.shape) # (1, 4, H, W)\n",
    "    break  # iterating once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "30d001ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx= 2008\n",
    "input_img, target_img =ds[idx]\n",
    "input_img= input_img.unsqueeze(0) # Adding batch dimension\n",
    "date=str(inputs_merged.time.values[idx]) #What date?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eabc5ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_instance.eval()\n",
    "with torch.no_grad():\n",
    "    pred_img = model_instance(input_img).squeeze(0).cpu().numpy()  \n",
    "target_img = target_img.cpu().numpy()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8be976b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descale_precip(x, min_val, max_val):\n",
    "    return x * (max_val - min_val) + min_val\n",
    "\n",
    "def descale_temp(x, mean, std):\n",
    "    return x * std + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d57c416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling params loading from the .json files\n",
    "scaling_dir = os.path.join(BASE_DIR, \"sasthana/Downscaling/Downscaling_Models/Training_Chronological_Dataset\")\n",
    "rhiresd_params = json.load(open(os.path.join(scaling_dir, \"RhiresD_scaling_params_chronological.json\")))\n",
    "tabsd_params   = json.load(open(os.path.join(scaling_dir, \"TabsD_scaling_params_chronological.json\")))\n",
    "tmind_params   = json.load(open(os.path.join(scaling_dir, \"TminD_scaling_params_chronological.json\")))\n",
    "tmaxd_params   = json.load(open(os.path.join(scaling_dir, \"TmaxD_scaling_params_chronological.json\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a5f9bb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_img_denorm = np.empty_like(pred_img)\n",
    "target_img_denorm = np.empty_like(target_img)\n",
    "\n",
    "pred_img_denorm[0] = descale_precip(pred_img[0], rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "pred_img_denorm[1] = descale_temp(pred_img[1], tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "pred_img_denorm[2] = descale_temp(pred_img[2], tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "pred_img_denorm[3] = descale_temp(pred_img[3], tmaxd_params[\"mean\"], tmaxd_params[\"std\"])\n",
    "\n",
    "target_img_denorm[0] = descale_precip(target_img[0], rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "target_img_denorm[1] = descale_temp(target_img[1], tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "target_img_denorm[2] = descale_temp(target_img[2], tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "target_img_denorm[3] = descale_temp(target_img[3], tmaxd_params[\"mean\"], tmaxd_params[\"std\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68805c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = inputs_merged.lat.values\n",
    "lon = inputs_merged.lon.values\n",
    "var_names = [\"RhiresD\", \"TabsD\", \"TminD\", \"TmaxD\"]\n",
    "# Define fixed colorbar limits for each variable\n",
    "vmin_dict = {\"RhiresD\": 0, \"TabsD\": -5, \"TminD\": -10, \"TmaxD\": -10}\n",
    "vmax_dict = {\"RhiresD\":100, \"TabsD\": 25, \"TminD\": 20, \"TmaxD\": 30}\n",
    "\n",
    "swiss_extent = [5.9, 10.5, 45.7, 47.9]  \n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(18, 18), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "input_img = input_img.squeeze(0)  \n",
    "for i, var in enumerate(var_names):\n",
    "    # Get input image for this variable (input_img shape: (5, H, W))\n",
    "    input_img_denorm = None\n",
    "    if var == \"RhiresD\":\n",
    "        input_img_denorm = descale_precip(input_img[0].cpu().numpy(), rhiresd_params[\"min\"], rhiresd_params[\"max\"])\n",
    "    elif var == \"TabsD\":\n",
    "        input_img_denorm = descale_temp(input_img[1].cpu().numpy(), tabsd_params[\"mean\"], tabsd_params[\"std\"])\n",
    "    elif var == \"TminD\":\n",
    "        input_img_denorm = descale_temp(input_img[2].cpu().numpy(), tmind_params[\"mean\"], tmind_params[\"std\"])\n",
    "    elif var == \"TmaxD\":\n",
    "        input_img_denorm = descale_temp(input_img[3].cpu().numpy(), tmaxd_params[\"mean\"], tmaxd_params[\"std\"])\n",
    "\n",
    "    vmin = min(target_img_denorm[i].min(), pred_img_denorm[i].min(), input_img_denorm.min())\n",
    "    vmax = max(target_img_denorm[i].max(), pred_img_denorm[i].max(), input_img_denorm.max())\n",
    "\n",
    "    # Input\n",
    "    ax = axes[i, 0]\n",
    "    im = ax.pcolormesh(lon, lat, input_img_denorm, cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Input {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Target\n",
    "    ax = axes[i, 1]\n",
    "    im = ax.pcolormesh(lon, lat, target_img_denorm[i], cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Target {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Predicted\n",
    "    ax = axes[i, 2]\n",
    "    im = ax.pcolormesh(lon, lat, pred_img_denorm[i], cmap='coolwarm', vmin=vmin, vmax=vmax, shading='auto', transform=ccrs.PlateCarree())\n",
    "    ax.set_title(f\"Predicted {var}\")\n",
    "    ax.set_extent(swiss_extent, crs=ccrs.PlateCarree())\n",
    "    ax.coastlines(resolution='10m')\n",
    "    ax.add_feature(cfeature.BORDERS)\n",
    "    fig.colorbar(im, ax=ax, orientation='vertical', fraction=0.046, pad=0.04)\n",
    "plt.suptitle(f\"Inputs(bicubic) , Target(HR) and Predictions from the 1971 time series : {date}\", fontsize=14, y=0.95)\n",
    "plt.tight_layout()\n",
    "plt.savefig(BASE_DIR+ \"sasthana/Downscaling/Processing_and_Analysis_Scripts/Outputs/1_July_2016_unet_1971_2020.png\", dpi=1000, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1865da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
