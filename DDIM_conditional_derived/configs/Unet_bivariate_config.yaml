defaults:
  - paths: paths_12km_bivariate.yaml
  - _self_

wandb:
  project: DDIM_residual_run_12km_bivariate
  name: ${hydra.job.override_dirname}

experiment:
  batch_size: 32
  num_workers: 4

variables:
  input:
    precip: RhiresD
    temp: TabsD
  target:
    precip: RhiresD
    temp: TabsD

preprocessing:
  nan_to_num: true
  nan_value : 0.0

data:
  train:
    input: ${paths.data.train.input}
    target: ${paths.data.train.target}
  val:
    input: ${paths.data.val.input}
    target: ${paths.data.val.target}
  test:
    input: ${paths.data.test.input}
    target: ${paths.data.test.target}
  static:
    elevation: ${paths.data.static.elevation}

datamodule:
  _target_: DDIM_conditional_derived.DownscalingDataModule.DownscalingDataModule
  train_input: ${data.train.input}
  train_target: ${data.train.target}
  val_input: ${data.val.input}
  val_target: ${data.val.target}
  test_input: ${data.test.input}
  test_target: ${data.test.target}
  elevation: ${data.static.elevation}
  batch_size: ${experiment.batch_size}
  num_workers: ${experiment.num_workers}
  preprocessing:
    variables: ${variables}
    preprocessing: ${preprocessing}

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    patience: 10
    mode: min

  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: DDIM_conditional_derived/trained_ckpts/12km/
    filename: ${model._target_}_bs${experiment.batch_size}_lr${model.lr}_timesteps${model.timesteps}_schedule${model.beta_schedule}.ckpt
    save_top_k: 1
    monitor: val/loss
    mode: min

model:
  _target_: DDIM_conditional_derived.models.diff_module.DDIMResidualContextual
  denoiser:
    _target_: DDIM_conditional_derived.models.components.diff.denoiser.unet.DDIMDenoiserUNet
    in_ch: 3
    out_ch: 2
    features: [64,128,256,512]
    channel_names: ["precip", "temp"]
  context_encoder:
    _target_: DDIM_conditional_derived.models.components.diff.conditioner.AFNOConditionerNetBase
    # add context encoder params as needed

    
  unet_regr:
    _target_: LDM_conditional.models.unet_module.DownscalingUnetLightning
    checkpoint: /path/to/pretrained/unet_regression.ckpt
  timesteps: 1000
  beta_schedule: "linear"
  linear_start: 1e-4
  linear_end: 2e-2
  lr: 1e-4
  use_ema: True
  parameterization: "eps"

lr_scheduler:
  _target_: lightning.pytorch.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.50
  patience: 3
  min_lr: 1e-6