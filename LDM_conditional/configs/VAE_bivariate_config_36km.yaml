defaults:
  - paths: paths_36km_bivariate.yaml
  - _self_

vae:
  latent_dim: 32
  kl_weight: 0.01
  ae_flag: "residual"
  beta_anneal_steps: 1000
  encoder_levels: 2
  encoder_min_ch: 16
  encoder_ch_mult: 4
  decoder_levels: 2
  decoder_min_ch: 16
  lr: 0.001
  batch_size: 32
  num_workers: 4
  scheduler_factor: 0.5
  scheduler_patience: 3

experiment:
  batch_size: ${vae.batch_size}
  num_workers: ${vae.num_workers}

variables:
  input:
    precip: RhiresD
    temp: TabsD
  target:
    precip: RhiresD
    temp: TabsD
preprocessing:
  nan_to_num: true
  nan_value : 0.0

data:
  train:
    input: ${paths.data.train.input}
    target: ${paths.data.train.target}
  val:
    input: ${paths.data.val.input}
    target: ${paths.data.val.target}
  test:
    input: ${paths.data.test.input}
    target: ${paths.data.test.target}
  static:
    elevation: ${paths.data.static.elevation}

encoder:
  _target_: LDM_conditional.models.components.ae.SimpleConvEncoder
  in_dim: 2
  levels: ${vae.encoder_levels}
  min_ch: ${vae.encoder_min_ch}
  ch_mult: ${vae.encoder_ch_mult}

decoder:
  _target_: LDM_conditional.models.components.ae.SimpleConvDecoder
  in_dim: ${vae.latent_dim}
  levels: ${vae.decoder_levels}
  min_ch: ${vae.decoder_min_ch}
  out_dim: 2


datamodule:
  _target_: LDM_conditional.DownscalingDataModule.DownscalingDataModule
  train_input: ${data.train.input}
  train_target: ${data.train.target}
  val_input: ${data.val.input}
  val_target: ${data.val.target}
  test_input: ${data.test.input}
  test_target: ${data.test.target}
  elevation: ${data.static.elevation}
  batch_size: ${experiment.batch_size}
  num_workers: ${experiment.num_workers}
  preprocessing:
    variables: ${variables}
    preprocessing: ${preprocessing}

callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    patience: 10
    mode: min
    
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: LDM_conditional/trained_ckpts_optimised/36km/VAE_ckpts/
    filename: VAE_levels_latentdim_${vae.latent_dim}_klweight_${vae.kl_weight}_checkpoint
    save_top_k: 1
    monitor: val/loss
    mode: min

unet_arch:
  _target_: LDM_conditional.models.unet_module.DownscalingUnet
  in_ch: 3  #2 vars + elevation
  out_ch: 2
  features: [64,128,256,512]


model:
  _target_: LDM_conditional.models.ae_module.AutoencoderKL
  encoder: ${encoder}
  decoder: ${decoder}
  latent_dim: ${vae.latent_dim}
  kl_weight: ${vae.kl_weight}
  ae_flag: ${vae.ae_flag}
  beta_anneal_steps: ${vae.beta_anneal_steps}

  unet_regr: /Users/sasthana/curnagl_shivanshi/Downscaling/Downscaling_Models/LDM_conditional/trained_ckpts_optimised/36km/LDM_conditional.models.unet_module.DownscalingUnetLightning_36km_logtransform_lr0.001_precip_loss_weight5.0_1.0_crps[0, 1]_factor0.5_pat3.ckpt.ckpt

lr_scheduler:
  _target_: lightning.pytorch.lr_scheduler.ReduceLROnPlateau
  mode: min
  factor: 0.50
  patience: 3
  min_lr: 1e-6