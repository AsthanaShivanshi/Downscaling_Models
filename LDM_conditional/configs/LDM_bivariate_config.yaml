defaults:
  - paths: paths_12km_bivariate.yaml
  - _self_


latent_dim: 64  #Corr to optimum VAE latent dim : AsthanaSh


experiment:
  batch_size: 32
  num_workers: 4

variables:
  input:
    precip: RhiresD
    temp: TabsD

  target:
    precip: RhiresD
    temp: TabsD

preprocessing:
  nan_to_num: true
  nan_value : 0.0

data:
  train:
    input: ${paths.data.train.input}
    target: ${paths.data.train.target}
  val:
    input: ${paths.data.val.input}
    target: ${paths.data.val.target}
  test:
    input: ${paths.data.test.input}
    target: ${paths.data.test.target}
  static:
    elevation: ${paths.data.static.elevation}

encoder:
  _target_: LDM_conditional.models.components.ae.SimpleConvEncoder
  in_dim: 2
  levels: 2
  min_ch: 16
  ch_mult: 4

decoder:
  _target_: LDM_conditional.models.components.ae.SimpleConvDecoder
  in_dim: ${latent_dim}
  levels: 2
  min_ch: 16
  out_dim: 2

conditioner:
  _target_: LDM_conditional.models.components.ldm.conditioner.AFNOConditionerNetCascade
  autoencoder: ${model.autoencoder}
  embed_dim: [64,128,256,256]  #Must match the denoiser context_ch : AsthanaSh
  analysis_depth: 4
  cascade_depth: 4
  context_ch: [64,128,256,256]  #Must match the denoiser context_ch : AsthanaSh


#Denoising Unet should expect the dims of the VAE output (latent): AsthanaSh
denoiser:
  _target_: LDM_conditional.models.components.ldm.denoiser.UNetModel
  in_channels:  ${latent_dim} #Must match the VAE latent channels : AsthanaSh
  out_channels: ${latent_dim} 
  model_channels: 64
  num_res_blocks: 2
  attention_resolutions: [1,2,4]
  context_ch: [64,128,256,256]  #Must match the conditioner embed_dim : AsthanaSh
  channel_mult: [1,2,4,4]
  conv_resample: true
  dims: 2
  use_fp16: false
  num_heads: 4


sampler: #For inference only : AsthanaSh
  _target_: LDM_conditional.models.components.ldm.denoiser.DDIMSampler
  model: ${denoiser}
  schedule: "quadratic" 
  device: "cuda"
  ddim_num_steps: 1000
  ddim_eta: 0.0


ema:
  _target_: LDM_conditional.models.components.ldm.denoiser.LitEma
  model: ${denoiser}
  decay: 0.9999
  use_num_updates: true

datamodule:
  _target_: LDM_conditional.DownscalingDataModule.DownscalingDataModule #Path to the exact class, not the module file : AsthanaSh
  train_input: ${data.train.input}
  train_target: ${data.train.target}
  val_input: ${data.val.input}
  val_target: ${data.val.target}
  test_input: ${data.test.input}
  test_target: ${data.test.target}
  elevation: ${data.static.elevation} #Added elevation to datamodule for context conditioning : AsthanaSh
  batch_size: ${experiment.batch_size}
  num_workers: ${experiment.num_workers}
  preprocessing:
    variables: ${variables}
    preprocessing: ${preprocessing}

  
callbacks:
  early_stopping:
    _target_: lightning.pytorch.callbacks.EarlyStopping
    monitor: val/loss
    patience: 10
    mode: min
    
  model_checkpoint:
    _target_: lightning.pytorch.callbacks.ModelCheckpoint
    dirpath: trained_ckpts_optimised/LDM_ckpts/
    filename: LDM_ckpt_{model.beta_schedule}_cosines{model.cosine_s}_linearend{model.linear_end}_loss{model.loss_type}_latent{latent_dim}_checkpoint
    save_top_k: 1
    monitor: val/loss
    mode: min

model:
  _target_: LDM_conditional.models.ldm_module.LatentDiffusion
  denoiser: ${denoiser}
  autoencoder:
    _target_: LDM_conditional.models.ae_module.AutoencoderKL
    encoder: ${encoder}
    decoder: ${decoder}
    latent_dim: ${latent_dim}
  unet_regr: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/LDM_conditional/trained_ckpts_optimised/12km/LDM_conditional.models.unet_module.DownscalingUnetLightning_logtransform_lr0.01_precip_loss_weight5.0_1.0_crps[0, 1]_factor0.5_pat3.ckpt.ckpt
  
  context_encoder: ${conditioner}  #Conditions the denoiser with coarse UNet predictors: AsthanaSh
  #VAE ckpt loaded from the ae_load_state_file 
  ae_load_state_file: /work/FAC/FGSE/IDYST/tbeucler/downscaling/sasthana/Downscaling/Downscaling_Models/LDM_conditional/trained_ckpts_optimised/12km/VAE_ckpts/VAE_levels_latentdim_64_klweight_0.01_checkpoint.ckpt
  linear_start: 1e-4   
  linear_end: 1e-3 #or linear and quadratic : AsthanaSh
  cosine_s: 8e-3  #Only for cosine schedule : AsthanaSh
  #Note : Output from LDM : [batch_size, 4, height, width] (AsthanaSh)
  lr: 1e-4
  lr_patience: 3
  lr_factor: 0.5
  parameterization: "v"
  beta_schedule: "cosine"  #Could be "linear" or "cosine" or "quadratic"
  loss_type: "l2" #Could be "l1" or "l2 or crps,,,,check diffmodule
  timesteps: 1000